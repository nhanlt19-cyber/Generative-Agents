# So S√°nh Model: OpenAI vs Ollama

## ‚ùå Ollama KH√îNG C√≥ Model "gpt-3.5-turbo"

**L√Ω do:**
- `gpt-3.5-turbo` l√† model ƒë·ªôc quy·ªÅn c·ªßa OpenAI
- Ollama l√† platform m√£ ngu·ªìn m·ªü v·ªõi c√°c model ri√™ng
- Ollama kh√¥ng th·ªÉ ch·∫°y model c·ªßa OpenAI

## ‚úÖ Code ƒê√£ T·ª± ƒê·ªông X·ª≠ L√Ω

Code ƒë√£ ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ **t·ª± ƒë·ªông** d√πng model Ollama khi `llm_provider = "ollama"`:

### Trong `gpt_structure.py`:

```python
def ChatGPT_single_request(prompt):
    if llm_provider == "ollama":
        return ollama_chat_request(prompt, model=ollama_model_name)  # D√πng model Ollama
    else:
        completion = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",  # Ch·ªâ d√πng khi llm_provider == "openai"
            messages=[{"role": "user", "content": prompt}]
        )
        return completion["choices"][0]["message"]["content"]
```

**K·∫øt lu·∫≠n:** Khi `llm_provider = "ollama"`, code s·∫Ω **KH√îNG** g·ªçi "gpt-3.5-turbo", m√† s·∫Ω d√πng model t·ª´ `ollama_model_name` trong `utils.py`.

## üìä So S√°nh Model T∆∞∆°ng ƒê∆∞∆°ng

### GPT-3.5-Turbo (OpenAI)
- **K√≠ch th∆∞·ªõc:** ~175B parameters (kh√¥ng c√¥ng khai ch√≠nh x√°c)
- **Ch·∫•t l∆∞·ª£ng:** R·∫•t t·ªët cho general tasks
- **T·ªëc ƒë·ªô:** R·∫•t nhanh (cloud)
- **Chi ph√≠:** Tr·∫£ ph√≠ theo usage

### Model Ollama T∆∞∆°ng ƒê∆∞∆°ng

| Model Ollama | K√≠ch th∆∞·ªõc | Ch·∫•t l∆∞·ª£ng | T·ªëc ƒë·ªô | RAM c·∫ßn | So s√°nh v·ªõi GPT-3.5 |
|--------------|------------|------------|--------|---------|---------------------|
| **llama3.1** | ~8B/70B | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | 8GB/40GB | G·∫ßn b·∫±ng ho·∫∑c t·ªët h∆°n |
| **llama3** | ~8B/70B | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | 8GB/40GB | G·∫ßn b·∫±ng |
| **llama2** | ~7B/13B/70B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | 8GB/16GB/40GB | H∆°i k√©m h∆°n |
| **mistral** | ~7B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° | 8GB | T·ªët, nhanh |
| **qwen2.5** | ~7B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | 8GB | T·ªët, ƒëa ng√¥n ng·ªØ |

## üéØ Khuy·∫øn Ngh·ªã Model Cho Generative Agents

### Cho Ch·∫•t L∆∞·ª£ng T·ªët Nh·∫•t (T∆∞∆°ng ƒê∆∞∆°ng GPT-3.5):
```python
ollama_model_name = "llama3.1"  # Ho·∫∑c "llama3"
```
- ‚úÖ Ch·∫•t l∆∞·ª£ng g·∫ßn b·∫±ng GPT-3.5
- ‚úÖ Hi·ªÉu context t·ªët
- ‚úÖ Ph√π h·ª£p cho simulation ph·ª©c t·∫°p

### Cho T·ªëc ƒê·ªô (Nhanh H∆°n):
```python
ollama_model_name = "mistral"
```
- ‚úÖ Nhanh h∆°n
- ‚úÖ Ch·∫•t l∆∞·ª£ng t·ªët
- ‚úÖ Ph√π h·ª£p cho simulation nhanh

### Cho RAM H·∫°n Ch·∫ø:
```python
ollama_model_name = "mistral"  # 7B, c·∫ßn ~8GB RAM
# ho·∫∑c
ollama_model_name = "llama3:8b"  # 8B version
```

## üîç Ki·ªÉm Tra Model ƒêang D√πng

### Trong Code:
File `reverie/backend_server/utils.py`:
```python
ollama_model_name = "llama3.1"  # Model n√†y s·∫Ω ƒë∆∞·ª£c d√πng thay cho gpt-3.5-turbo
```

### Khi Ch·∫°y:
Code s·∫Ω t·ª± ƒë·ªông d√πng `ollama_model_name` thay v√¨ "gpt-3.5-turbo" khi:
- `llm_provider = "ollama"`

## üìù C√°c H√†m ƒê√£ ƒê∆∞·ª£c C·∫≠p Nh·∫≠t

T·∫•t c·∫£ c√°c h√†m sau ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªÉ d√πng Ollama model:

1. ‚úÖ `ChatGPT_single_request()` - D√πng `ollama_model_name`
2. ‚úÖ `ChatGPT_request()` - D√πng `ollama_model_name`
3. ‚úÖ `GPT4_request()` - D√πng `ollama_model_name` (c√≥ th·ªÉ d√πng model l·ªõn h∆°n)
4. ‚úÖ `GPT_request()` - D√πng `ollama_model_name`
5. ‚úÖ `get_embedding()` - D√πng `ollama_embedding_model_name`

## üöÄ C√°ch Thay ƒê·ªïi Model

### Thay ƒê·ªïi Model Text Generation:

```bash
# Tr√™n VM Ubuntu
nano /root/Generative-Agents/reverie/backend_server/utils.py
```

S·ª≠a:
```python
ollama_model_name = "llama3.1"  # Thay ƒë·ªïi model ·ªü ƒë√¢y
```

**C√°c l·ª±a ch·ªçn:**
- `"llama3.1"` - T·ªët nh·∫•t (khuy·∫øn ngh·ªã)
- `"llama3"` - T·ªët
- `"llama2"` - ·ªîn ƒë·ªãnh
- `"mistral"` - Nhanh
- `"qwen2.5"` - ƒêa ng√¥n ng·ªØ

### T·∫£i Model M·ªõi:

```bash
ollama pull llama3.1
# ho·∫∑c
ollama pull mistral
```

## ‚ö†Ô∏è L∆∞u √ù

1. **Model ph·∫£i ƒë∆∞·ª£c t·∫£i tr∆∞·ªõc:**
   ```bash
   ollama list  # Xem model ƒë√£ t·∫£i
   ollama pull <model_name>  # T·∫£i model m·ªõi
   ```

2. **Model name ph·∫£i ch√≠nh x√°c:**
   - D√πng t√™n ch√≠nh x√°c t·ª´ `ollama list`
   - V√≠ d·ª•: `llama3.1` kh√¥ng ph·∫£i `llama-3.1`

3. **RAM requirements:**
   - Model l·ªõn c·∫ßn nhi·ªÅu RAM
   - Ki·ªÉm tra: `free -h`

## üîÑ Migration t·ª´ OpenAI sang Ollama

### Tr∆∞·ªõc (OpenAI):
```python
llm_provider = "openai"
# Code s·∫Ω d√πng: "gpt-3.5-turbo"
```

### Sau (Ollama):
```python
llm_provider = "ollama"
ollama_model_name = "llama3.1"
# Code s·∫Ω t·ª± ƒë·ªông d√πng: "llama3.1" thay cho "gpt-3.5-turbo"
```

**Kh√¥ng c·∫ßn s·ª≠a code kh√°c!** Code ƒë√£ t·ª± ƒë·ªông x·ª≠ l√Ω.

## üìä Performance Comparison

### GPT-3.5-Turbo (OpenAI):
- Response time: ~1-3 gi√¢y
- Quality: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- Cost: $0.0015/1K tokens

### Llama3.1 (Ollama):
- Response time: ~2-10 gi√¢y (t√πy hardware)
- Quality: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- Cost: $0 (local)

### Mistral (Ollama):
- Response time: ~1-5 gi√¢y
- Quality: ‚≠ê‚≠ê‚≠ê‚≠ê
- Cost: $0 (local)

## ‚úÖ K·∫øt Lu·∫≠n

1. ‚úÖ **Ollama KH√îNG c√≥ gpt-3.5-turbo** - ƒê√¢y l√† b√¨nh th∆∞·ªùng
2. ‚úÖ **Code ƒë√£ t·ª± ƒë·ªông d√πng model Ollama** khi `llm_provider = "ollama"`
3. ‚úÖ **Llama3.1 l√† l·ª±a ch·ªçn t·ªët nh·∫•t** ƒë·ªÉ thay th·∫ø GPT-3.5-turbo
4. ‚úÖ **Kh√¥ng c·∫ßn s·ª≠a code** - Ch·ªâ c·∫ßn ƒë·∫∑t `llm_provider = "ollama"` v√† ch·ªçn model trong `utils.py`

## üéØ Khuy·∫øn Ngh·ªã

**Cho d·ª± √°n Generative Agents:**
```python
ollama_model_name = "llama3.1"  # T·ªët nh·∫•t, g·∫ßn b·∫±ng GPT-3.5
```

**N·∫øu c·∫ßn nhanh h∆°n:**
```python
ollama_model_name = "mistral"  # Nhanh, ch·∫•t l∆∞·ª£ng t·ªët
```

**N·∫øu RAM h·∫°n ch·∫ø:**
```python
ollama_model_name = "mistral"  # 7B, ch·ªâ c·∫ßn ~8GB RAM
```


