# Sá»­a Lá»—i "Ollama ERROR" Trong Simulation

## ğŸ” Váº¥n Äá»

Khi cháº¡y simulation, báº¡n tháº¥y:
```
Activity: Isabella is Ollama ERROR
```

## ğŸ” NguyÃªn NhÃ¢n CÃ³ Thá»ƒ

1. **Ollama khÃ´ng cháº¡y** hoáº·c khÃ´ng thá»ƒ káº¿t ná»‘i
2. **Model chÆ°a Ä‘Æ°á»£c táº£i** hoáº·c tÃªn model sai
3. **Timeout** - Request quÃ¡ lÃ¢u
4. **Connection refused** - Ollama khÃ´ng láº¯ng nghe trÃªn port 11434
5. **Model khÃ´ng há»— trá»£** má»™t sá»‘ tÃ­nh nÄƒng

## âœ… CÃ¡c BÆ°á»›c Kiá»ƒm Tra

### 1. Kiá»ƒm Tra Ollama Äang Cháº¡y

```bash
# TrÃªn Ubuntu VM
systemctl status ollama

# Hoáº·c
ps aux | grep ollama

# Kiá»ƒm tra port
netstat -tlnp | grep 11434
```

**Náº¿u khÃ´ng cháº¡y:**
```bash
systemctl start ollama
# hoáº·c
ollama serve
```

### 2. Kiá»ƒm Tra Model ÄÃ£ Táº£i

```bash
ollama list
```

**Pháº£i tháº¥y:**
- `llama3.1` (hoáº·c model báº¡n Ä‘ang dÃ¹ng)
- `nomic-embed-text` (cho embeddings)

**Náº¿u chÆ°a cÃ³:**
```bash
ollama pull llama3.1
ollama pull nomic-embed-text
```

### 3. Test Káº¿t Ná»‘i

```bash
# Test API
curl http://localhost:11434/api/tags

# Test chat
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.1",
  "messages": [{"role": "user", "content": "Hello"}],
  "stream": false
}'
```

### 4. Cháº¡y Debug Script

```bash
cd /root/Generative-Agents/reverie/backend_server
source /root/Generative-Agents/venv/bin/activate
python debug_ollama.py
```

Script nÃ y sáº½ test táº¥t cáº£ cÃ¡c chá»©c nÄƒng vÃ  hiá»ƒn thá»‹ lá»—i chi tiáº¿t.

### 5. Kiá»ƒm Tra Logs Chi Tiáº¿t

Khi cháº¡y simulation, xem terminal output Ä‘á»ƒ tháº¥y lá»—i chi tiáº¿t. Code Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»ƒ hiá»ƒn thá»‹:
- Connection errors
- Timeout errors  
- HTTP errors
- Chi tiáº¿t exception

## ğŸ”§ CÃ¡c Giáº£i PhÃ¡p

### Giáº£i PhÃ¡p 1: Restart Ollama

```bash
# Restart Ollama service
systemctl restart ollama

# Hoáº·c kill vÃ  start láº¡i
pkill ollama
ollama serve
```

### Giáº£i PhÃ¡p 2: Kiá»ƒm Tra Model Name

```bash
# Kiá»ƒm tra tÃªn model chÃ­nh xÃ¡c
ollama list

# Sá»­a trong utils.py náº¿u cáº§n
nano /root/Generative-Agents/reverie/backend_server/utils.py
```

Äáº£m báº£o `ollama_model_name` khá»›p vá»›i model Ä‘Ã£ táº£i.

### Giáº£i PhÃ¡p 3: TÄƒng Timeout

Náº¿u model cháº­m, cÃ³ thá»ƒ cáº§n tÄƒng timeout. Sá»­a trong `ollama_interface.py`:

```python
# Thay Ä‘á»•i tá»« 120 thÃ nh 300 (5 phÃºt)
response = requests.post(url, json=payload, timeout=300)
```

### Giáº£i PhÃ¡p 4: Kiá»ƒm Tra RAM

Model lá»›n cáº§n nhiá»u RAM. Kiá»ƒm tra:

```bash
free -h
```

Náº¿u RAM Ä‘áº§y, Ollama cÃ³ thá»ƒ khÃ´ng pháº£n há»“i.

### Giáº£i PhÃ¡p 5: DÃ¹ng Model Nhá» HÆ¡n

Náº¿u `llama3.1` quÃ¡ lá»›n, thá»­ model nhá» hÆ¡n:

```bash
ollama pull mistral
```

Sá»­a trong `utils.py`:
```python
ollama_model_name = "mistral"
```

## ğŸ“Š Debug Chi Tiáº¿t

### Xem Logs Ollama

```bash
# Xem logs systemd
journalctl -u ollama -f

# Hoáº·c náº¿u cháº¡y manual
ollama serve  # Xem output trá»±c tiáº¿p
```

### Test Thá»§ CÃ´ng

```python
# Trong Python
import requests

url = "http://localhost:11434/api/chat"
payload = {
    "model": "llama3.1",
    "messages": [{"role": "user", "content": "Hello"}],
    "stream": False
}

response = requests.post(url, json=payload, timeout=120)
print(response.status_code)
print(response.json())
```

## âœ… Checklist

- [ ] Ollama Ä‘ang cháº¡y (`systemctl status ollama`)
- [ ] Model Ä‘Ã£ táº£i (`ollama list`)
- [ ] CÃ³ thá»ƒ káº¿t ná»‘i (`curl http://localhost:11434/api/tags`)
- [ ] Model name Ä‘Ãºng trong `utils.py`
- [ ] Äá»§ RAM (`free -h`)
- [ ] Cháº¡y `debug_ollama.py` thÃ nh cÃ´ng
- [ ] Xem logs chi tiáº¿t khi cháº¡y simulation

## ğŸš€ Sau Khi Sá»­a

1. **Restart simulation server:**
```bash
# Dá»«ng server
screen -X -S sim_server quit

# Khá»Ÿi Ä‘á»™ng láº¡i
cd /root/Generative-Agents
source venv/bin/activate
cd reverie/backend_server
python reverie.py
```

2. **Cháº¡y láº¡i simulation:**
```
run 10  # Test vá»›i sá»‘ steps nhá» trÆ°á»›c
```

## ğŸ“ LÆ°u Ã

- Code Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»ƒ hiá»ƒn thá»‹ lá»—i chi tiáº¿t hÆ¡n
- Cháº¡y `debug_ollama.py` Ä‘á»ƒ test trÆ°á»›c khi cháº¡y simulation
- Náº¿u váº«n lá»—i, xem terminal output Ä‘á»ƒ tháº¥y chi tiáº¿t exception

